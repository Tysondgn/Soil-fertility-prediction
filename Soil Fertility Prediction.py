# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9dKrskauiXk-Wir8VIkYrKMEjygzOS-

# Soil Fertility Prediction

## Soil Fertility Chemical Content Required Prediction
"""

import pandas as pd
import numpy as np

df = pd.read_csv("crop.csv")

"""N = Nitrogen

P = phosphorous

K = Potassium

Ph = A scale used to identify acidity or basicity nature; (Acid Nature- Ph<7;
Neutral- Ph=7; Base Nature-P>7)

EC = Electrical Conductivity

OC = Organic Carbon

S = Sulphur

zn = zinc

Fe = iron

Cu = Copper

Mn = Magnesium

B = Boron


"""

df

"""## Data visualization of Seperate column on histogram"""

# plotting modules
import seaborn as sns
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(24, 15))
i=0
for column in df:
        sub = fig.add_subplot(3, 5, i + 1)
        sub.set_xlabel('value')
        # Set plot labels and title
        plt.ylabel('Frequency')
        plt.title(column)
        df[column] .plot(kind = 'hist' )
        i= i + 1

# bar plot
cat_list = ['N','P','K']
fig = plt.figure(figsize = (16,8))
for i in range(len(cat_list)):
     column = cat_list[i]
     sub = fig.add_subplot(2, 4, i + 1)
     chart = sns.countplot(data = df,x= column, hue= 'Output', palette = 'RdYlBu')

"""## Under Sampling (output data 0,1)"""

from imblearn.under_sampling import RandomUnderSampler

# Specify the column for under-sampling
target_column = 'Output'  # Replace 'your_target_column' with the actual column name

# Create the under-sampler
sampler = RandomUnderSampler(random_state=42)

# Perform under-sampling
X_resampled, y_resampled = sampler.fit_resample(df.drop(columns=[target_column]), df[target_column])

# Construct the new DataFrame
df_resampled = pd.concat([pd.DataFrame(X_resampled), pd.Series(y_resampled, name=target_column)], axis=1)

# Display the class distribution after under-sampling
print(df_resampled[target_column].value_counts())

df = df_resampled



"""## Over Sampling Using SMOTE for ( Output variable 2 )"""

from imblearn.over_sampling import SMOTE

# Separate the feature variables and the target variable
X = df.drop('Output', axis=1)  # Replace 'target_variable' with the actual column name
y = df['Output']

# Perform oversampling using SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new DataFrame with the oversampled data
oversampled_df = pd.concat([pd.DataFrame(X_resampled), pd.Series(y_resampled)], axis=1)
oversampled_df.columns = df.columns

# Display the class distribution after under-sampling
print(oversampled_df['Output'].value_counts())

# Specify the column for which you want to create a histogram
column_name = 'Output'  # Replace 'col_name' with the actual column name

# Create the histogram plot
plt.hist(oversampled_df[column_name], bins=30, alpha=0.5, color='steelblue')

# Set plot labels and title
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram of Column: {}'.format(column_name))

# Display the plot
plt.show()

df = oversampled_df

"""## Data Distribution"""

X = df.iloc[:, :-1].values
X

Y = df.iloc[:, -1].values
Y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Linear Regression"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state = 0)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix: ")
print(cm)

print("Accuracy of the Model: {0}%".format(accuracy_score(y_test, y_pred)*100))

"""## Random Forest"""

# Fitting Random Forest Regression to the dataset
# import the regressor
from sklearn.ensemble import RandomForestRegressor

# create regressor object
regressor = RandomForestRegressor(n_estimators=100, random_state=0)

# fit the regressor with x and y data
regressor.fit(X, Y)

y_pred = regressor.predict(X_test)
y_pred = np.round(y_pred)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix: ")
print(cm)

print("Accuracy of the Model: {0}%".format(accuracy_score(y_test, y_pred)*100))

"""## K-Nearest Neighbour"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train, y_train)

# Predict on dataset which model has not seen before
print(knn.predict(X_test))

y_pred = knn.predict(X_test)
y_pred = np.round(y_pred)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix: ")
print(cm)

print("Accuracy of the Model: {0}%".format(accuracy_score(y_test, y_pred)*100))

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Creating the classifier object
clf_gini = DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=3, min_samples_leaf=5)

# Performing training
clf_gini.fit(X_train, y_train)

# Decision tree with entropy
clf_entropy = DecisionTreeClassifier(
            criterion = "entropy", random_state = 100,
            max_depth = 3, min_samples_leaf = 5)

# Performing training
clf_entropy.fit(X_train, y_train)

y_pred = clf_gini.predict(X_test)
print("Predicted values:")
print(y_pred)

print("Confusion Matrix: ",confusion_matrix(y_test, y_pred))

print("Accuracy : ",accuracy_score(y_test,y_pred)*100)

from sklearn.metrics import classification_report
print("Report : ",classification_report(y_test, y_pred))

"""## Artificial Neural Network"""

import keras
from keras.models import Sequential
from keras.layers import Dense

classifier = Sequential()

"""**Added the input layer and the first hidden layer**






*  Sequential Class. units represents the number of hidden neurons in the hidden layer.

*  The activation function in the hidden layer for a fully connected neural network should be the Rectifier Activation function. That’s why I use ‘relu’.

*  we have 12 independent variables,That’s why input_dim = 12.
"""

classifier.add(Dense(units = 6,  kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))

"""Added the second hidden layer"""

classifier.add(Dense(units = 6,  kernel_initializer = 'uniform', activation = 'relu'))

"""Add the output layer"""

classifier.add(Dense(units = 1,  kernel_initializer = 'uniform', activation = 'sigmoid'))

"""**Train ANN**

The training part requires two steps- Compile the ANN, and Fit the ANN to the Training set.

Compile ANN
"""

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

"""Fit ANN to Training set"""

classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)

"""Predicting Test Result"""

y_pred = classifier.predict(X_test)
y_pred = np.round(y_pred)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test,y_pred)

"""## SVM"""

from sklearn.svm import SVC
svc = SVC()
svc.fit(X_train, y_train)
y_pred  = svc.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test,y_pred)

"""## XG Boost

"""

import xgboost as xgb

# Create the XGBoost classifier
xgb_model = xgb.XGBClassifier()

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)