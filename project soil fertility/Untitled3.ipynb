{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNZla56UdPUA"
   },
   "source": [
    "# Soil Fertility Chemical Content Required Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b01pmeiQApy7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zgOCXH0XBUpw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"crop.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HmFQeANA1t7"
   },
   "source": [
    "N = Nitrogen\n",
    "\n",
    "P = phosphorous\n",
    "\n",
    "K = Potassium\n",
    "\n",
    "Ph = A scale used to identify acidity or basicity nature; (Acid Nature- Ph<7; \n",
    "Neutral- Ph=7; Base Nature-P>7)\n",
    "\n",
    "EC = Electrical Conductivity\n",
    "\n",
    "OC = Organic Carbon\n",
    "\n",
    "S = Sulphur\n",
    "\n",
    "zn = zinc\n",
    "\n",
    "Fe = iron\n",
    "\n",
    "Cu = Copper\n",
    "\n",
    "Mn = Magnesium\n",
    "\n",
    "B = Boron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "eC9VmclECWEn",
    "outputId": "371c6069-1fbc-414c-e107-9995140292f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>351</td>\n",
       "      <td>10.7</td>\n",
       "      <td>623</td>\n",
       "      <td>7.96</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>11.03</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>264</td>\n",
       "      <td>9.0</td>\n",
       "      <td>486</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.35</td>\n",
       "      <td>8.26</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>276</td>\n",
       "      <td>9.2</td>\n",
       "      <td>370</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>320</td>\n",
       "      <td>13.8</td>\n",
       "      <td>391</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.07</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>264</td>\n",
       "      <td>10.3</td>\n",
       "      <td>475</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.45</td>\n",
       "      <td>7.36</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       N     P    K    pH    EC    OC      S    Zn    Fe    Cu     Mn     B  \\\n",
       "0    138   8.6  560  7.46  0.62  0.70   5.90  0.24  0.31  0.77   8.71  0.11   \n",
       "1    213   7.5  338  7.62  0.75  1.06  25.40  0.30  0.86  1.54   2.89  2.29   \n",
       "2    163   9.6  718  7.59  0.51  1.11  14.30  0.30  0.86  1.57   2.70  2.03   \n",
       "3    157   6.8  475  7.64  0.58  0.94  26.00  0.34  0.54  1.53   2.65  1.82   \n",
       "4    270   9.9  444  7.63  0.40  0.86  11.80  0.25  0.76  1.69   2.43  2.26   \n",
       "..   ...   ...  ...   ...   ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "875  351  10.7  623  7.96  0.51  0.29   7.24  0.36  4.69  0.69  11.03  0.69   \n",
       "876  264   9.0  486  7.24  0.47  0.10   3.92  0.35  8.26  0.45   7.98  0.40   \n",
       "877  276   9.2  370  7.62  0.62  0.49   6.64  0.42  3.57  0.63   6.48  0.32   \n",
       "878  320  13.8  391  7.38  0.65  1.07   5.43  0.58  4.58  1.02  13.25  0.53   \n",
       "879  264  10.3  475  7.49  0.74  0.88  10.56  0.45  7.36  1.87  10.63  0.63   \n",
       "\n",
       "     Output  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         1  \n",
       "..      ...  \n",
       "875       1  \n",
       "876       1  \n",
       "877       1  \n",
       "878       2  \n",
       "879       0  \n",
       "\n",
       "[880 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MHP8lXOTHuC",
    "outputId": "3bd11060-8750-4d28-d452-b9df25f013e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.380e+02, 8.600e+00, 5.600e+02, ..., 7.700e-01, 8.710e+00,\n",
       "        1.100e-01],\n",
       "       [2.130e+02, 7.500e+00, 3.380e+02, ..., 1.540e+00, 2.890e+00,\n",
       "        2.290e+00],\n",
       "       [1.630e+02, 9.600e+00, 7.180e+02, ..., 1.570e+00, 2.700e+00,\n",
       "        2.030e+00],\n",
       "       ...,\n",
       "       [2.760e+02, 9.200e+00, 3.700e+02, ..., 6.300e-01, 6.480e+00,\n",
       "        3.200e-01],\n",
       "       [3.200e+02, 1.380e+01, 3.910e+02, ..., 1.020e+00, 1.325e+01,\n",
       "        5.300e-01],\n",
       "       [2.640e+02, 1.030e+01, 4.750e+02, ..., 1.870e+00, 1.063e+01,\n",
       "        6.300e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWPrjRjPdH-l",
    "outputId": "888ffda0-f2b4-40ef-d6a7-2fc972440f4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 2,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 2, 1, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1,\n",
       "       1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 2, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df.iloc[:, -1].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "P48__zmyVigu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1ZY7r98iVz5J"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sjigcg-Joe8"
   },
   "source": [
    "## Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "L0feU7XXWMe5",
    "outputId": "9ca1b76e-9593-41a5-fbc8-cd1eb5ff2a8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state = 0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-U8YNYI4Wnkl",
    "outputId": "427680ec-7ee8-47bc-8755-7a0768452404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcsG6RJqacbw",
    "outputId": "cff590fe-5220-4f00-95ca-6620aa7906f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[96  9  0]\n",
      " [ 6 96  1]\n",
      " [ 0 11  1]]\n",
      "Accuracy of the Model: 87.72727272727273%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "\n",
    "print(\"Accuracy of the Model: {0}%\".format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEJOJoDrL8F-"
   },
   "source": [
    "## Random Forest Regression /classification change/ dataset change using smoke technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "ITB_cXNXeBPV",
    "outputId": "5327688c-fcce-4025-e352-c1174c9e1ad9",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Random Forest Regression to the dataset\n",
    "# import the regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create regressor object\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# fit the regressor with x and y data\n",
    "regressor.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQyR46ZZNYog",
    "outputId": "d371447d-d030-412c-d59c-e2c47753c165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 2.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 2.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 2.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 2.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 2.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "weWAm5vMN_YF",
    "outputId": "8324cceb-b103-4e7b-af46-9884446dd375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[15 90  0]\n",
      " [17 86  0]\n",
      " [ 1 11  0]]\n",
      "Accuracy of the Model: 45.909090909090914%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "\n",
    "print(\"Accuracy of the Model: {0}%\".format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgULphQURxUF"
   },
   "source": [
    "## K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DepwC6MEQW3M",
    "outputId": "6ea62da3-bd23-4d52-a33c-9805a3dfbce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0\n",
      " 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0\n",
      " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "  \n",
    "knn.fit(X_train, y_train)\n",
    "  \n",
    "# Predict on dataset which model has not seen before\n",
    "print(knn.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8liBDHyRmBe",
    "outputId": "7ece5f47-ea2e-4074-e6ed-b18bb120229d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueGaQchFRXUL",
    "outputId": "03b33671-beb3-441e-9f20-c95776e660df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[88 17  0]\n",
      " [14 89  0]\n",
      " [ 0 12  0]]\n",
      "Accuracy of the Model: 80.45454545454545%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "\n",
    "print(\"Accuracy of the Model: {0}%\".format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVkh8Qy4VSbh"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V1RgFd-hR59W",
    "outputId": "97ab6630-3cbf-452a-b17f-2cb780173924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "[0 0 1 1 2 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 2 0 1 0 0 0 1 1 1 0 2 2 1 0 0 0 1\n",
      " 1 0 2 1 1 0 0 0 1 1 1 0 2 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0\n",
      " 0 2 0 1 0 0 1 1 0 2 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 2 0 1 0\n",
      " 1 1 0 1 1 0 1 0 0 0 2 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 2 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0]\n",
      "Confusion Matrix:  [[98  7  0]\n",
      " [ 7 93  3]\n",
      " [ 0  4  8]]\n",
      "Accuracy :  90.45454545454545\n",
      "Report :                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       105\n",
      "           1       0.89      0.90      0.90       103\n",
      "           2       0.73      0.67      0.70        12\n",
      "\n",
      "    accuracy                           0.90       220\n",
      "   macro avg       0.85      0.83      0.84       220\n",
      "weighted avg       0.90      0.90      0.90       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Creating the classifier object\n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\",random_state = 100,max_depth=3, min_samples_leaf=5)\n",
    "  \n",
    "# Performing training\n",
    "clf_gini.fit(X_train, y_train)\n",
    "\n",
    "# Decision tree with entropy\n",
    "clf_entropy = DecisionTreeClassifier(\n",
    "            criterion = \"entropy\", random_state = 100,\n",
    "            max_depth = 3, min_samples_leaf = 5)\n",
    "  \n",
    "# Performing training\n",
    "clf_entropy.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "print(\"Predicted values:\")\n",
    "print(y_pred)\n",
    "\n",
    "print(\"Confusion Matrix: \",confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "print(\"Accuracy : \",accuracy_score(y_test,y_pred)*100)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Report : \",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1MyF2P3WnY1"
   },
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EQn2JecOVWgN"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwZiTncFW46L"
   },
   "source": [
    "**Added the input layer and the first hidden layer**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*  Sequential Class. output_dim represents the number of hidden neurons in the hidden layer.\n",
    "\n",
    "*  The activation function in the hidden layer for a fully connected neural network should be the Rectifier Activation function. That’s why I use ‘relu’.\n",
    "\n",
    "*  we have 12 independent variables,That’s why input_dim = 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gsclpK4yW3qB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier\u001b[38;5;241m.\u001b[39madd(Dense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,  kernel_initializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(units = 6,  kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUeXldLaYmjS"
   },
   "source": [
    "Added the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "A9JZkYP6XDrv"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier\u001b[38;5;241m.\u001b[39madd(Dense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,  kernel_initializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(units = 6,  kernel_initializer = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UmyO664YtnK"
   },
   "source": [
    "Add the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hPMPlkhfYuMD"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1,  kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9PYpKxNZLoe"
   },
   "source": [
    "**Train ANN**\n",
    "\n",
    "The training part requires two steps- Compile the ANN, and Fit the ANN to the Training set.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUFisGouZeuC"
   },
   "source": [
    "Compile ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bD0c7A2XZeL0"
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h5c_PYSdEQc"
   },
   "source": [
    "Fit ANN to Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQ-8Gi0UZLTu",
    "outputId": "c705d805-c04c-4c88-d4fd-3aca1ec7a414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/66 [==============================] - 1s 1ms/step - loss: 0.6909 - accuracy: 0.5045\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.6676 - accuracy: 0.5106\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.5839 - accuracy: 0.5106\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.5485\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3568 - accuracy: 0.7773\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2643 - accuracy: 0.8242\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.1864 - accuracy: 0.8485\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.8576\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.8667\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.0322 - accuracy: 0.8682\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.1062 - accuracy: 0.8712\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.1857 - accuracy: 0.8758\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.2645 - accuracy: 0.8773\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.3565 - accuracy: 0.8818\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.4521 - accuracy: 0.8833\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.5545 - accuracy: 0.8879\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.6667 - accuracy: 0.8864\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.7883 - accuracy: 0.8909\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -0.9182 - accuracy: 0.8894\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -1.0628 - accuracy: 0.8909\n",
      "Epoch 21/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -1.2176 - accuracy: 0.8939\n",
      "Epoch 22/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -1.3860 - accuracy: 0.8939\n",
      "Epoch 23/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -1.5767 - accuracy: 0.8939\n",
      "Epoch 24/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -1.7672 - accuracy: 0.8939\n",
      "Epoch 25/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -1.9857 - accuracy: 0.8939\n",
      "Epoch 26/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -2.2068 - accuracy: 0.8939\n",
      "Epoch 27/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -2.4559 - accuracy: 0.8909\n",
      "Epoch 28/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -2.6984 - accuracy: 0.8924\n",
      "Epoch 29/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -2.9859 - accuracy: 0.8909\n",
      "Epoch 30/100\n",
      "66/66 [==============================] - 0s 2ms/step - loss: -3.2832 - accuracy: 0.8955\n",
      "Epoch 31/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -3.6042 - accuracy: 0.8939\n",
      "Epoch 32/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -3.9328 - accuracy: 0.8939\n",
      "Epoch 33/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -4.2793 - accuracy: 0.8924\n",
      "Epoch 34/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -4.6432 - accuracy: 0.8955\n",
      "Epoch 35/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -5.0179 - accuracy: 0.8955\n",
      "Epoch 36/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -5.4252 - accuracy: 0.8955\n",
      "Epoch 37/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -5.8245 - accuracy: 0.8939\n",
      "Epoch 38/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -6.2706 - accuracy: 0.8939\n",
      "Epoch 39/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -6.7286 - accuracy: 0.8894\n",
      "Epoch 40/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -7.1946 - accuracy: 0.8924\n",
      "Epoch 41/100\n",
      "66/66 [==============================] - 0s 2ms/step - loss: -7.6977 - accuracy: 0.8924\n",
      "Epoch 42/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -8.2148 - accuracy: 0.8955\n",
      "Epoch 43/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -8.7486 - accuracy: 0.8924\n",
      "Epoch 44/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -9.2818 - accuracy: 0.8924\n",
      "Epoch 45/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -9.8789 - accuracy: 0.8894\n",
      "Epoch 46/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -10.4526 - accuracy: 0.8939\n",
      "Epoch 47/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -11.0750 - accuracy: 0.8939\n",
      "Epoch 48/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -11.6820 - accuracy: 0.8955\n",
      "Epoch 49/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -12.3424 - accuracy: 0.8939\n",
      "Epoch 50/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -13.0290 - accuracy: 0.8909\n",
      "Epoch 51/100\n",
      "66/66 [==============================] - 0s 2ms/step - loss: -13.7040 - accuracy: 0.8924\n",
      "Epoch 52/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -14.3996 - accuracy: 0.8924\n",
      "Epoch 53/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -15.1597 - accuracy: 0.8924\n",
      "Epoch 54/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -15.8708 - accuracy: 0.8909\n",
      "Epoch 55/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -16.6538 - accuracy: 0.8970\n",
      "Epoch 56/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -17.4314 - accuracy: 0.8894\n",
      "Epoch 57/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -18.2772 - accuracy: 0.8924\n",
      "Epoch 58/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -19.1107 - accuracy: 0.8909\n",
      "Epoch 59/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -19.9974 - accuracy: 0.8939\n",
      "Epoch 60/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -20.9118 - accuracy: 0.8924\n",
      "Epoch 61/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -21.8289 - accuracy: 0.8939\n",
      "Epoch 62/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -22.7830 - accuracy: 0.8894\n",
      "Epoch 63/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -23.6780 - accuracy: 0.8894\n",
      "Epoch 64/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -24.6970 - accuracy: 0.8970\n",
      "Epoch 65/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -25.6712 - accuracy: 0.8939\n",
      "Epoch 66/100\n",
      "66/66 [==============================] - 0s 2ms/step - loss: -26.7158 - accuracy: 0.8879\n",
      "Epoch 67/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -27.7283 - accuracy: 0.8955\n",
      "Epoch 68/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -28.8202 - accuracy: 0.8939\n",
      "Epoch 69/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -29.9340 - accuracy: 0.8909\n",
      "Epoch 70/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -31.0258 - accuracy: 0.8864\n",
      "Epoch 71/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -32.1718 - accuracy: 0.8924\n",
      "Epoch 72/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -33.3009 - accuracy: 0.8939\n",
      "Epoch 73/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -34.5073 - accuracy: 0.8924\n",
      "Epoch 74/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -35.7425 - accuracy: 0.8939\n",
      "Epoch 75/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -37.0086 - accuracy: 0.8939\n",
      "Epoch 76/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -38.2843 - accuracy: 0.8970\n",
      "Epoch 77/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -39.5058 - accuracy: 0.8879\n",
      "Epoch 78/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -40.8261 - accuracy: 0.8909\n",
      "Epoch 79/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -42.2135 - accuracy: 0.8955\n",
      "Epoch 80/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -43.5404 - accuracy: 0.8924\n",
      "Epoch 81/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -44.9077 - accuracy: 0.8894\n",
      "Epoch 82/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -46.3054 - accuracy: 0.8909\n",
      "Epoch 83/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -47.7430 - accuracy: 0.8939\n",
      "Epoch 84/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -49.2117 - accuracy: 0.8924\n",
      "Epoch 85/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -50.7195 - accuracy: 0.8909\n",
      "Epoch 86/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -52.2633 - accuracy: 0.8939\n",
      "Epoch 87/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -53.7892 - accuracy: 0.8939\n",
      "Epoch 88/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -55.3564 - accuracy: 0.8894\n",
      "Epoch 89/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -57.0049 - accuracy: 0.8924\n",
      "Epoch 90/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -58.6544 - accuracy: 0.8939\n",
      "Epoch 91/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -60.4066 - accuracy: 0.8924\n",
      "Epoch 92/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -62.1409 - accuracy: 0.8970\n",
      "Epoch 93/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -63.8235 - accuracy: 0.8909\n",
      "Epoch 94/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -65.6515 - accuracy: 0.8955\n",
      "Epoch 95/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -67.4971 - accuracy: 0.8970\n",
      "Epoch 96/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -69.2381 - accuracy: 0.8909\n",
      "Epoch 97/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -71.2633 - accuracy: 0.8924\n",
      "Epoch 98/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -73.0895 - accuracy: 0.8939\n",
      "Epoch 99/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -75.0163 - accuracy: 0.8939\n",
      "Epoch 100/100\n",
      "66/66 [==============================] - 0s 1ms/step - loss: -76.9874 - accuracy: 0.8909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b16d97cd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GroUMcrZq1a"
   },
   "source": [
    "Predicting Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WK8vDcyPZuUk",
    "outputId": "b4d382e2-964a-45db-da1c-3f271e25720a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 2.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 2.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 2.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcrWG9b8b-lf",
    "outputId": "245f2564-1e58-4b9e-f796-084aa485353a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95 10  0]\n",
      " [ 8 95  0]\n",
      " [ 0 12  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8636363636363636"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nNZla56UdPUA",
    "4Sjigcg-Joe8",
    "gEJOJoDrL8F-",
    "KgULphQURxUF",
    "pVkh8Qy4VSbh",
    "L1MyF2P3WnY1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
